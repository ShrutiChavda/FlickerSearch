Tutorial 0

1. TensorFlow 
 Description: An open-source machine learning library developed by Google, 
used for building and deploying ML and deep learning models. 
 Use Cases: Image recognition, NLP, recommendation systems, and robotics. 
 Key Features: Scalable across platforms, supports hardware acceleration 
(GPUs/TPUs), integrates with TensorFlow Lite and TensorFlow.js for mobile and 
web deployment. 
2. PyTorch 
 Description: A deep learning library developed by Facebook, known for its 
dynamic computation graph and ease of experimentation. 
 Use Cases: Research in AI, computer vision, NLP, and reinforcement learning. 
 Key Features: Dynamic computation graphs, GPU acceleration, native Python 
integration, and TorchServe for model deployment. 
3. Keras 
 Description: A high-level deep learning API built on TensorFlow for rapid 
prototyping of neural networks. 
 Use Cases: Neural network design, image classification, and text generation. 
 Key Features: User-friendly, modular, supports multiple backends, and 
simplifies complex model building. 
4. OpenCV 
 Description: An open-source library for computer vision and image processing 
tasks.
 Use Cases: Face detection, object tracking, image filtering, and AR 
applications. 
 Key Features: Extensive image and video processing functions, real-time 
operations, and multi-platform support. 
5. Scikit-Learn 
 Description: A Python library for classical machine learning algorithms and 
data preprocessing. 
 Use Cases: Predictive analytics, clustering, and regression analysis. 
 Key Features: Easy-to-use API, robust implementation of ML algorithms, and 
excellent integration with NumPy and pandas. 
6. NLTK (Natural Language Toolkit) 
 Description: A Python library for NLP tasks, widely used in academic and 
industrial research. 
 Use Cases: Text tokenization, sentiment analysis, and speech tagging. 
 Key Features: Extensive NLP datasets and tools, compatibility with Python, 
and educational resources. 
7. SpaCy 
 Description: A Python-based library for advanced natural language processing. 
 Use Cases: Named entity recognition, part-of-speech tagging, and dependency 
parsing. 
 Key Features: Fast, efficient, pretrained models for multiple languages, and 
seamless integration with other AI tools. 
8. GPT-3 (OpenAI) 
 Description: A state-of-the-art language model for generating human-like text. 
 Use Cases: Chatbots, content creation, summarization, and translation. 
 Key Features: Extensive pretraining, high versatility, and the ability to generate 
coherent and contextually relevant responses. 
9. IBM Watson 
 Description: A suite of AI services and tools by IBM, offering cloud-based AI 
solutions. 
 Use Cases: Virtual assistants, business analytics, and healthcare diagnostics. 
 Key Features: Prebuilt APIs for NLP, computer vision, and speech recognition, 
with enterprise-level security. 
10. RapidMiner 
 Description: A platform for data science and machine learning workflows, 
targeting non-coders and experts alike. 
 Use Cases: Data preprocessing, predictive modeling, and business analytics. 
 Key Features: Drag-and-drop interface, end-to-end ML pipeline support, and 
integration with multiple data sources. 
_____________________________________________________________________

Tutorial 1

1. Import numpy library.
import numpy as np
2. Create 1D array with 5 elements.
x = np.array([1,2,3,4,5])
print(x)
3. Create 1D array with 5 elements random value. (by default values between 0 to 1)
x=np.random.rand(5)
print(x)
4. Generate random value from range 5 to 15 with 1D array size 10.
x=np.random.randint(5,16,10)
print(x)
5. Create 2D array - 4 by 3 size.
# x=np.array([[1,2,3],[1,2,3],[1,2,3],[1,2,3]])
# print(x)
x=np.random.randint(5,16,(4,3))
print(x)
6. Create 2D array with size 3 by 3. Elements values are random number(by default values between 0 to 1)
x=np.random.rand(3,3)
print(x)
7. Generate random value from range 5 to 15 with 2D array size 3 by 3.
x=np.random.randint(5,16,(3,3))
print(x)
8. Create 3D array with size 2 * 3 * 2.
#x = np.array([[[1,2],[1,2],[1,2]],[[1,2],[1,2],[1,2]]])
#print(x)
x=np.random.randint(5,16,(2,3,2))
print(x)
9. Initialize 5 by 5 array with all values are zeros.
# x=np.random.randint(0,1,(5,5))
# print(x)
x=np.zeros((5,5),dtype=int)
print(x)
10. Initialize 5 by 5 array with all values are ones.
# x=np.random.randint(1,2,(5,5))
# print(x)
x=np.ones((5,5),dtype=int)
print(x)
11. Initialize 5 by 5 array with all values are particular values(consider 4).
# x=np.random.randint(4,5,(5,5))
# print(x)
# x=np.random.randint(4,5,(5,5))
# print(x)
x=np.full((5,5),4)
print(x)
12. Create indentity matrix with all diagonal values are 1 and rest all values are zeros.
x=np.eye(5,5,dtype=int)
print(x)
13. Find out shape of array.
x.shape
14. Find dimension of array.
x.ndim
15. Find no. of elements of from above any array.
x.size
16. Find type of array.
x.dtype
17. Find maximum value from array.
# a=np.max(x)
# print(a)
a=np.max([1,2,3,4,5])
print(a)
18. Find minimum value from array.
# x=np.min(x)
# print(x)
x=np.min([1,2,3,4,5])
print(x)
19. Find average (mean) values from array.
a = np.array([1,2,3])
b = a.mean()
print(b)
20. Find location of maximum value from array.
# x = np.array([[[1,2],[1,2],[1,2]],[[1,2],[1,2],[1,2]]])  #2 * 3 * 2
# print(x)
a=np.argmax(x)
print(a)
21. Create two 2D array of size 3 by 3 and perform four basic mathematical operations.
a = np.random.randint(1,6,(3,3))
b = np.random.randint(1,6,(3,3))
print(a)
print(b)
print(a+b)
print(a-b)
print(a*b)
print(a/b)
22. Print index 2 and -2 value for 1D array. (indexing).
a = np.array([1,2,3,4,5])
print(a)
print('second element',a[2])
print('second last element',a[-2])
23. Perform indexing operations with 2D array.
# x = np.array([[1,2,3],[1,2,3],[1,2,3]])
# print(x)
b = x[2] = 10
print(b)
print(x)
24. Print value of index 2 to 5 (slicing)
x = np.array([10,20,30,40,50,60,70,80,90.100],dtype=int)
print(x)
b = x[2:6]
print(b)
25. Print from index 0 to 5 values
b = x[0:6]
print(b)
26. Print from index 5 to all
b = x[5:]
print(b)
27. Reassign value for particular index.
c = x[0] = 100
print(x)
28.  Reassign value 100 to all elements.
c = x[0:] = 100
print(x)
29. Extract elements from index 1 to 7 with a step of 2.
x = np.array([1,2,3,4,5,6,7,8,9,10])
b = x[1:8:2]
print(b)
30. Print Reverse the array.
b = x[::-1]
print(b)

_____________________________________________________________________

Tutorial 2

Explore Pandas library to analyze  and manipulate data.
Description: Use Pandas library for data analysis and manipulation.

Dataset: Sample CSV file

Task: Load the dataset, perform data cleaning, manipulation (like sorting, filtering), and exploratory data analysis.


1. Import numpy and pandas library.
import numpy as np
import pandas as pd
2. Define series with 3 elements.
x = pd.Series([1,2,3])
print(x)
3. Define a series with a custom index.
x1 = pd.Series([1,2,3,4,5],["A","B","C","D","E"])
print(x1)
4. Fetch Series value and index.
print(x1.values)
print(x1.index)
5. Create DataFrame from list of Dict.
data = [
    {'Name':'abc','Age':20},
    {'Name':'def','Age':21},
    {'Name':'ghi','Age':22}
]
data
6. Create sample excel file in your computer and upload it to drive. Read the excel file in colab.
pip install openpyxl
x2 = pd.read_excel("Student.xlsx")
x2
7. Explore Kaggle: Your Machine Learning and Data Science Community and UCI Machine Learning Repository for a dataset. Search iris dataset and download.
8. Read iris dataset from csv file format and findout no. of rows and columns.
x3 = pd.read_csv('iris.csv')
x3
9. Perform the following operation on the iris dataset.
    a. Fetch the first 5 and last 5 rows.
    x3.head()
    b. Find index.
    x3.index
    c. Find column name and rename it with SepalLengthCm':'Sepal_Length','SepalWidthCm':'Sepal_Width','PetalLengthCm':'Petal_Length','PetalWidthCm':'Petal_Width'
    # x3.columns
    x3=x3.rename(columns={'Sepal.Length':'Sepal_length','Sepal.Width':'Sepal_Width','Petal.Length':'Petal_Length','Petal.Width':'Petal_Width'})
    x3
    #find the categories
    categories = x3['Species'].unique()
    print(categories)
    d. replace "Iris-versicolor" with "versicolor", same way do for rest 2 categories.
    x3.replace("Iris-versicolor","versicolor",inplace=True)
    x3.replace("Iris-setosa","setosa",inplace=True)
    x3.replace("Iris-virginica","virginica",inplace=True)
    categories
    e. Find Statistical description.
    x3.describe()
    f. extract specific column.
    x3[['Id','Sepal_length']].head(3)
    g. add new column with condition.
    def yes_no(Sepal_length):
    return 'YES' if Sepal_length >= 5 else "No"
    x3['Result'] = x3['Sepal_length'].apply(yes_no)
    x3
    h. drop added column.
    # x3.drop('Result',axis=1,inplace=True)
    x3
    i. select first row data.
    x3.iloc[0]
    j. select 51 to 55 rows
    x3.iloc[51:56]
    k. Apply conditional formatting.
    x3[x3['Petal_Length']>5]
    # x3[x3['Species'] == 'virginica'].count()
    l. print "virginica" with "Petal_Length" more than 6.
    x3[(x3['Petal_Length']>6) & (x3['Species'] == 'virginica')]
    m. find duplicate values.
    x3.duplicated()
    # x3.duplicated().sum()
    n. Find basic information.
    x3.info()
    o. Find null value in dataset.
    x3.isna()
    # x3.isna().sum()
10. Download student_info dataset. Find null value from it. And fill null value using mean of that column.
Dataset Link: https://drive.google.com/drive/folders/1xTZizwH2onTZ8KpW1gttiFZ8SrY8RKOt?usp=sharing
std = pd.read_csv('student_info.csv')
std
# find the null values and fill it
std['study_hours'] = std['study_hours'].fillna(std['study_hours'].mean())
std.isna().sum()
11. Import adult data set from UCIML Adult - UCI Machine Learning Repository. And perform following task.
    a. Find missing value. Find type of missing data.
    #   pip install ucimlrepo
    from ucimlrepo import fetch_ucirepo
    #fetch the dataset
    adult = fetch_ucirepo(id=2)
    #data
    X = adult.data.features
    y = adult.data.targets
    print(adult.metadata)
    #variable information
    print(adult.variables)
    b. Handle missing value.
    X.isna().sum()
    # X['workclass'].unique()
    cat = X['workclass'].unique()
    print(cat)
    X['workclass'].mode()  #The mode is a statistical measure that represents the most frequently occurring value in a dataset.
    X['workclass'].fillna(X['workclass'].mode()[0],inplace=True)
    X.isna().sum()

    X['occupation'].unique()
    X['occupation'].mode()
    X['occupation'].fillna(X['occupation'].mode()[0], inplace=True)
    X.isna().sum()
    
    X['native-country'].unique()
    X['native-country'].mode()
    X['native-country'].fillna(X['native-country'].mode()[0],inplace=True)
    X.isna().sum()
_____________________________________________________________________


Tutorial 3 

Visualize Data Using Matplotlib and Seaborn Library
Description: Create visualizations with Matplotlib and Seaborn. 

Dataset: Clean_Dataset 

Task: Create various plots (e.g., histograms, scatter plots, box plots) to visualize the data.

1. Import numpy, pandas, seaborn and matplotlib library.
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
2. Download data set from kaggle : Flight Price Prediction (kaggle.com)
3. Read dataset “Clean_Dataset”.
cd = pd.read_csv("Clean_Dataset.csv")
cd
4. Find Out columns.
cd.columns
5. Rename columns Unnamed: 0'  to 'id'.
cd = cd.rename(columns={'Unnamed: 0':'id'})
cd.columns
6. Are there any null values in the dataset?
cd.isna().sum()
7. Find categories of ‘source_city’. (categorical data)
cd['source_city'].unique()
8. Find categories of ‘class’. (categorical data)
cd['class'].unique()
9. Draw Count plot for ‘source_city’.
sns.countplot(y='source_city',data=cd)
10. Draw Count plot for ‘class’.
sns.countplot(x='class',data=cd)
11. Draw a pie plot for ‘source_city’.
cd['source_city'].value_counts().plot(kind='pie',autopct='%.2f')
12. Draw Donut plot for ‘source_city’.
plt.pie(cd['source_city'].value_counts(),labels=cd['source_city'].unique(),autopct='%.2f',wedgeprops=dict(width=0.4))
13. Draw Histogram plot for ‘price’.
plt.hist(cd['price'])
14. Draw Distplot for ‘price’
sns.distplot(cd['price'])


___________________________________________________________________________

Tutorial 4

Implement Simple Linear Regression

Use the computers.csv dataset and perform the following activities:
Dataset contains the details of time taken to repair a computer, given the number of units to be repaired.
Dataset: Download from drive link
    import numpy as np
    import pandas as pd
    import matplotlib.pyplot as plt
    %matplotlib inline
    cm = pd.read_csv('computers.csv')
    cm
    #Feature Engineering
    Step 1: Find the mean
    mean_value = cm['Minutes'].mean()
    print(mean_value)
    Step 2: Plot into graph
    plt.scatter(cm['Units'], cm['Minutes'], color='blue', marker='o')
    plt.axhline(y=mean_value, c='r')
    plt.xlabel('Units')
    plt.ylabel('Minutes')
    Step - 3. Creating three Linear Regression Models
    - model 0 - set mean value
    - model 1 - 10 + (12 * Units)
    - model 2 - 6 + (18 * Units)
    plt.scatter(cm['Units'], cm['Minutes'], color='blue', marker='o')
    plt.axhline(y=mean_value, c='r')
    plt.xlabel('Units')
    plt.ylabel('Minutes')
    #Creating three Linear Regression Models
    minutes_model0 = cm['Minutes'].mean()
    minutes_model1 = 10 + 12*cm['Units']
    minutes_model2 = 6 + 18*cm['Units']
    Step - 4. Add model0, model1, model2 columns to dataset
    cm['min_model0'] = minutes_model0
    cm['min_model1'] = minutes_model1
    cm['min_model2'] = minutes_model2
    cm
    Step - 5. Create a sub plot.
    fig, ax = plt.subplots()

    #plotting the actual 'Minutes'
    ax.scatter(x="Units", y="Minutes", data=computers, label='Actual repair time')

    #plotting the model 0 predictions
    ax.plot(computers['Units'], computers['min_model0'], color='red', label='model0')

    #plotting the model 1 predictions
    ax.plot(computers['Units'], computers['min_model1'], color='green', label='model1')

    #plotting the model 2 predictions
    ax.plot(computers['Units'], computers['min_model2'], color='black', label='model2')

    #Adding xlabel, ylabel, title and legend
    ax.set_ylabel('Minutes')
    ax.set_xlabel('Units')
    ax.set_title('Speculated Models')
    ax.legend()

    Step 6. Do a linear regression
    - set X(input)=Unites and y(output)=Minutes
    - importing a reqierd class
    - creating a linear regression model
    - fitting the model
    - fetch intercept and coefficient

1. Build Regression model using Scikit-Learn Library(Model 3).
    a. Set the 'Units' column as the input data or predictor column
    X = cm[['Units']]
    b. Set the 'Minutes' column as the output data
    y = cm['Minutes']
    c. Importing the required class -  LinearRegression
    from sklearn.linear_model import LinearRegression
    d. Creating a linear regression model
    model = LinearRegression()
    e. Fitting the model to the data
    model.fit(X, y)
    f. Fetching intercept and coefficient
    print("Intercept: ", model.intercept_)
    print("Coefficient: ", model.coef_)
2. Creating Linear Regression model with calculated coefficient and intercept.
3. Add the above model to the dataframe for Visualization.
minutes_model3 = model.intercept_ + model.coef_*cm['Units']
cm['min_model3'] = minutes_model3
cm
4. Visualize Models using pyplot.
fig, ax = plt.subplots()

#plotting the actual 'Minutes'
ax.scatter(x="Units", y="Minutes", data=cm, label='Actual repair time')

#plotting the model 0 predictions
ax.plot(cm['Units'], cm['min_model0'], color='red', label='model0')

#plotting the model 1 predictions
ax.plot(cm['Units'], cm['min_model1'], color='green', label='model1')

#plotting the model 2 predictions
ax.plot(cm['Units'], cm['min_model2'], color='black', label='model2')

#plotting the model 3 predictions
ax.plot(cm['Units'], cm['min_model3'], color='yellow', label='model3')

#Adding xlabel, ylabel, title and legend
ax.set_ylabel('Minutes')
ax.set_xlabel('Units')
ax.set_title('Speculated Models')
ax.legend()
5. Compute the Coefficient of Determination to find accuracy.
Rsq = model.score(computers[['Units']], y)
Rsq

__________________________________________________________________

Tutorial 5

Multiple Linear Regression Model

std_marks_data.csv

To understand Multiple Linear Regression, let us consider the student marks dataset.

This model helps us to predict the marks of the students using the previous dataset.
The inputs are hours(time spent to study) ,age(present age) and internet(weather the internet is available or not), by using these 3 input fields we are predicting the marks of the students.
1. Observe data and do some preprocessing.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
%matplotlib inline
marks = pd.read_csv('std_marks_data.csv')
marks
marks.info()
2. Find no. of missing values.
marks.isna().sum()
3. Fill mean value in place of NaN value so NaN value does not affect accuracy.
marks.hours = marks.hours.fillna(marks.hours.mean())
marks.isna().sum()
4. Segregate input and output.
X = marks.iloc[:,:-1]
X
y = marks.iloc[:,-1]
y
5. Prepare model.
from sklearn.linear_model import LinearRegression
model = LinearRegression()
model.fit(X,Y)
print('Intercept = ',model.intercept_)
print('Coefficients = ',model.coef_)
6. Test model with new input data.
hours = int(input("Enter study hours: "))
age = int(input("Enter age: "))
internet = int(input("Enter internet usage: "))
print(hours)
print(age)
print(internet)
new_input = pd.DataFrame({'hours':[hours],'age':[age],'internet':[internet]})
print(new_input)
new_output = model.predict(new_input)
print(new_output)
print("Predicted Marks = ",new_output[0])

______________________________________________________________________________


Tutorial 6

Build Logistic Regression Model
Consider a coronary heart disease dataset (chd_data.csv) which lists the age in years ('age') and the presence/absence of evidence of significant coronary heart disease ('chd') for 100 patients.
The variable chd = 0 indicates the absence of coronary heart disease, whereas chd=1 indicates the presence of coronary heart disease.

1. Import dataset and visualize the data to get an insight on building the model.
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
chd_data = pd.read_csv('chd_data.csv')
chd_data
2. Importing the required class
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
3. Specifying the columns as predictor and target variable
# Visualize the data using a scatter plot
plt.scatter(chd_data['age'], chd_data['chd'])
plt.xlabel('Age')
plt.ylabel('Presence of CHD')
plt.title('Scatter Plot of Age vs. CHD')
plt.show()
X = chd_data[['age']]  # Predictor variable (age)
y = chd_data['chd']   # Target variable (chd)
4. Split the data in training and test set in 70:30 ratio.
# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
5. Built model using sklearn.linear_model.LogisticRegression class.
# Build the logistic regression model
model = LogisticRegression()
6. Train the model using the training data.
# Train the model using the training data
model.fit(X_train, y_train)
7. Fetch the intercept and the coefficients of the model.
intercept = model.intercept_
coefficients = model.coef_
print("Intercept:", intercept)
print("Coefficients:", coefficients)
8. Creating sample data.
sample_data = pd.DataFrame({'age': [45, 55, 65]})
9. Predicting the probabilities for each of the class labels.
probabilities = model.predict_proba(sample_data)
print("Predicted Probabilities:\n", probabilities)
10. Evaluate the model's performance on training and test data using 'accuracy' measure.
# Evaluate the model's performance on training and test data
y_train_pred = model.predict(X_train)
y_test_pred = model.predict(X_test)
train_accuracy = accuracy_score(y_train, y_train_pred)
test_accuracy = accuracy_score(y_test, y_test_pred)
print("Train Accuracy:", train_accuracy)
print("Test Accuracy:", test_accuracy)

______________________________________________________________________________________


Tutorial 7 

K-Nearest Neighbor

Let us implement the algorithm with the help of the 'Mobile.csv' dataset.
It contains data about customers “Age”, “EstimatedSalary”, “Purchased” .
Download the dataset from the shared folder.


Predict if the customer will purchase a mobile or not.

1. Load data : mobile.csv
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, classification_report
df = pd.read_csv('mobile.csv')
df
2. Look inside the dataset by finding information.
print("Dataset Information:")
print(df.info())  # Display information about the DataFrame (data types, non-null counts, etc.)
print("\nFirst 5 rows of the dataset:")
print(df.head())  # Display the first 5 rows of the DataFrame
print("\nDescriptive statistics:")
print(df.describe()) # Display statistical summary of numerical columns
print("\nUnique values in Purchased column:")
print(df['Purchased'].unique()) # Display unique values of the target variable
3. Draw scatter plot of purchased and not_purchased value with respect to Age and EstimatedSalary.
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1) # Create the first subplot
purchased = df[df['Purchased'] == 1] # Filter rows where 'Purchased' is 1
not_purchased = df[df['Purchased'] == 0] # Filter rows where 'Purchased' is 0
plt.scatter(purchased['Age'], purchased['EstimatedSalary'], color='green', label='Purchased') # Scatter plot for purchased data
plt.scatter(not_purchased['Age'], not_purchased['EstimatedSalary'], color='red', label='Not Purchased') # Scatter plot for not purchased data
plt.title('Age vs EstimatedSalary (Purchased)') # Set the title of the plot
plt.xlabel('Age') # Set the x-axis label
plt.ylabel('EstimatedSalary') # Set the y-axis label
plt.legend() # Display the legend

plt.subplot(1,2,2) # Create the second subplot
plt.scatter(purchased['Age'], purchased['Purchased'], color='green', label='Purchased') # Scatter plot of age vs purchase
plt.scatter(not_purchased['Age'], not_purchased['Purchased'], color='red', label='Not Purchased') # Scatter plot of age vs purchase
plt.title('Age vs Purchased') # Set the title
plt.xlabel('Age') # Set the x label
plt.ylabel('Purchased') # Set the y label
plt.legend() # Display the legend

plt.show() # Show the plots
4. Do Feature engineering (normalize the Age and EstimatedSalary columns)
scaler = StandardScaler() # Initialize StandardScaler
df[['Age', 'EstimatedSalary']] = scaler.fit_transform(df[['Age', 'EstimatedSalary']])
X = df[['Age', 'EstimatedSalary']] # Features
y = df['Purchased'] # Target variable
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
5. Split the dataset into train and test data
k_values = [3, 5, 7] # Define the values of k to test
train_accuracies = {} # Dictionary to store training accuracies
test_accuracies = {} # Dictionary to store testing accuracies

6. Build the kNN model with k=5,k=3,k=7.
for k in k_values:
    knn = KNeighborsClassifier(n_neighbors=k) # Initialize KNeighborsClassifier with k neighbors
    knn.fit(X_train, y_train) # Train the model

    # Evaluate on the training set
    y_train_pred = knn.predict(X_train) # Predict on the training set
    train_accuracy = accuracy_score(y_train, y_train_pred) # Calculate training accuracy
    train_accuracies[k] = train_accuracy # Store training accuracy

    # Evaluate on the test set
    y_test_pred = knn.predict(X_test) # Predict on the test set
    test_accuracy = accuracy_score(y_test, y_test_pred) # Calculate testing accuracy
    test_accuracies[k] = test_accuracy # Store testing accuracy

    print(f"\nResults for k={k}:")
    print(f"  Train Accuracy: {train_accuracy:.4f}")
    print(f"  Test Accuracy: {test_accuracy:.4f}")
    print(f"  Classification Report:\n{classification_report(y_test, y_test_pred)}")

7. Evaluate model performance on train and test sets with different value of k.
print("\nSummary of Train Accuracies:")
for k, accuracy in train_accuracies.items():
    print(f"  k={k}: {accuracy:.4f}") # Print training accuracies for each k

print("\nSummary of Test Accuracies:")
for k, accuracy in test_accuracies.items():
    print(f"  k={k}: {accuracy:.4f}")
______________________________________________________________________________________


Tutorial 8 

SVM for classification

Consider the Iris dataset which provides measurements of sepal length, sepal width, petal length, and petal width for 50 flowers from each of 3 species. Total rows are 150.

1. Create Binary Class SVM model
    1. Reading input from csv file (iris.csv)
    import numpy as np
    import pandas as pd
    import matplotlib.pyplot as plt
    import seaborn as sns
    df = pd.read_csv('Iris.csv')
    df
    2. Do Feature Engineering
    v_nv_fn = lambda x: 0 if x=="Iris-versicolor" else 1
    3. Create a new column in the dataframe (v_nv), that distinguishes the species - 'versicolor'(marked by 0) from rest.
    df["v_nv"] = df["Species"].apply(v_nv_fn)
    df
    4. Build Model
    sns.pairplot(df,
             x_vars = "PetalLengthCm",
             y_vars="PetalWidthCm",
             hue="v_nv",height=5)
    5. Visualize Model using mlxtend
    from sklearn.svm import SVC
    X=df[["PetalLengthCm","PetalWidthCm"]]
    y=df["v_nv"]
    model=SVC()
    model.fit(X,y)
    model.score(X,y)

2. Create Multi Class SVM model
    1. Reading input from csv file (iris.csv)
    pip install mlxtend
    df = pd.read_csv('Iris.csv')
    df
    from mlxtend.plotting import plot_decision_regions
    plt.title('Decision boundary of SVM on iris data')
    2. Do Feature Engineering
    features = np.array(X)
    target = np.array(y)
    plot_decision_regions(features,target,clf=model)
    plt.xlabel("PetalLengthCm")
    plt.ylabel("PetalWidthCm")
    df.Species = df.Species.astype(object)
    3. Encode the species column with numerical values. And replace label 'setosa' with '0', 'versicolor' with '1' and 'virginica' with '2'.
    df.loc[df.Species=="Iris-setosa","Species"]= 0
    df.loc[df.Species=="Iris-versicolor","Species"]= 1
    df.loc[df.Species=="Iris-virginica","Species"]= 2
    df.Species=df.Species.astype("category")
    df
    4. Build Multiclass Model using SVM
    X=df[['PetalLengthCm','PetalWidthCm']]
    y=df["Species"]
    model=SVC()
    model.fit(X,y)
    model.score(X,y)
    5. Visualize Model using mlxtend
    from mlxtend.plotting import plot_decision_regions
    features = np.array(X)
    target = np.array(y)
    plot_decision_regions(features,target,clf=model)
    plt.xlabel("PetalLengthCm")
    plt.ylabel("PetalWidthCm")
    plt.title('Multiclass classification on iris using SVM')
______________________________________________________________________________________


Tutorial 9 

Decision Tree

1. Load dataset users.csv
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline
user = pd.read_csv('users.csv')
user
2. Assign predictors and targets.
X=user.columns.drop('Purchased')
y=user['Purchased']
3. Apply encoding using get_dummies().
user_encoded = pd.get_dummies(user[X]).astype(int)
user_encoded
4. Split the dataset into train and test data.
from sklearn.model_selection import train_test_split
X_train,X_test,y_train,y_test = train_test_split(user_encoded, y, test_size=0.15, random_state=0)
5. Building the model using a decision tree.
from sklearn.tree import DecisionTreeClassifier
model = DecisionTreeClassifier(criterion='entropy',random_state=1)
model.fit(X_train,y_train)
train_predictions = model.predict(X_train)
test_predictions = model.predict(X_test)
6. Draw decision tree.
from sklearn.tree import plot_tree
#Plot the decision tree
plt.figure(figsize=(20,20))
plot_tree(
    model,
    filled=True,
    rounded=True,
    fontsize=12,
    feature_names=user_encoded.columns,
    class_names=['Not Purchased','Purchased']
)
plt.title("Decision Tree Visualization")
plt.show()
7. Evaluate model performance on train and test setosa#accuracy score
from sklearn.metrics import accuracy_score
y_pred_test = model.predict(X_test)
accuracy_score(y_test, y_pred_test)
# Train set performance
y_pred_train = model.predict(X_train)
print("Train Accuracy:", accuracy_score(y_train, y_pred_train))
# Test set performance
print("Test Accuracy:", accuracy_score(y_test, y_pred_test))

______________________________________________________________________________________

Tutorial 10 

Ensemble Learning

1. we are going to work on spambase dataset, where the normalized frequency of different words in an email are recorded, based on which an email is labeled as spam (1) or not spam (0)
#importing required libraries
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
%matplotlib inline
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import seaborn as sns
import warnings
warnings.filterwarnings("ignore")
2. Read dataset “spambase.csv”.
#reading input CSV file
spam_data=pd.read_csv('spambase.csv')
3. Split the data into a train and test set.
#split the dataset into features and target
features=spam_data.columns.drop('spam')
target='spam'
X = spam_data.iloc[:,:-1]
Y = spam_data.iloc[:,-1]
X_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.2,random_state=100)
4. Implement Bagging using Random Forest Algorithm.
#Method 1: Train Random Forest (Bagging)
from sklearn.ensemble import RandomForestClassifier
model = RandomForestClassifier(n_estimators=10, min_samples_split=20, min_impurity_decrease=0.05)
model.fit(X_train, Y_train)
train_accuracy = model.score(X_train, Y_train)
test_accuracy = model.score(X_test, Y_test)
print(train_accuracy,test_accuracy)
5. Implement Boosting using Adaboost Algorithm.
# Method 2: Boosting using Adaboost Algorithm
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import AdaBoostClassifier
model = AdaBoostClassifier(n_estimators=10)
model.fit(X_train, Y_train)
train_accuracy = model.score(X_train, Y_train)
test_accuracy = model.score(X_test, Y_test)
print(train_accuracy, test_accuracy)

#Evaluate the AdaBoost model using accuracy and classification report
Y_pred = model.predict(X_test)
accuracy = accuracy_score(Y_test, Y_pred)
report = classification_report(Y_test, Y_pred)
print("Accuracy:", accuracy)
print(report)

______________________________________________________________________________________

Tutorial 11 

Naive Bayes classification

1. Download and read dataset iris.csv.
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt 
iris = pd.read_csv("Iris.csv")
2. Import necessary files to implement Gaussian Naive Bayes classification.
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import classification_report, confusion_matrix
3. Perform Label encoding with required columns.
X = iris.iloc[:, :-1]  
y = iris.iloc[:, -1]  
4. Implement Gaussian Naive Bayes classification.
y = y.astype('category').cat.codes
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
model = GaussianNB()
model.fit(X_train, y_train)
y_pred = model.predict(X_test) 
5. Print confusion matrix and classification report.
print("\nClassification Report:")
print(classification_report(y_test, y_pred)) 
conf_matrix = confusion_matrix(y_test, y_pred)

sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')

plt.title("Confusion Matrix (Local Iris Dataset)")

plt.xlabel("Predicted")

plt.ylabel("Actual")

plt.show()

______________________________________________________________________________________


Tutorial 12 Implement kmeans clustering


Read dataset iris from sklearn online dataset.
2.1 Implement kmeans clusterting using sklearn with ELBOW method.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.cluster import KMeans
from sklearn.datasets import load_iris
from sklearn.preprocessing import StandardScaler
iris = load_iris()
dt = pd.DataFrame(iris.data, columns=iris.feature_names)
dt.columns
X = dt[['sepal length (cm)', 'sepal width (cm)']].values
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
# ELBOW METHOD to determine optimal k
wcss = []  # Within-cluster sum of squares
K = range(1, 11)
for k in K:
    kmeans = KMeans(n_clusters=k, init='k-means++', random_state=42)
    kmeans.fit(X_scaled)
    wcss.append(kmeans.inertia_)  # Append WCSS value
plt.figure(figsize=(8, 6))
plt.plot(K, wcss, marker='o', linestyle='-', color='b')
plt.xlabel('Number of Clusters (k)')
plt.ylabel('WCSS (Within-Cluster Sum of Squares)')
plt.title('Elbow Method for Optimal k')
plt.show()
kmeans = KMeans(n_clusters=3, init='k-means++', random_state=42)
y_kmeans = kmeans.fit_predict(X_scaled)
plt.figure(figsize=(8,6))
sns.scatterplot(x=X_scaled[:, 0], y=X_scaled[:, 1], hue=y_kmeans, palette='viridis')
plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], 
            s=300, c='red', marker='X', label='Centroids')
plt.xlabel('Sepal Length (Standardized)')
plt.ylabel('Sepal Width (Standardized)')
plt.title('K-Means Clustering of Iris Dataset')
plt.legend()
plt.show()

2.1 Implement kmeans clusterting using sklearn without ELBOW method.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.cluster import KMeans
from sklearn.datasets import load_iris
from sklearn.preprocessing import StandardScaler
iris = load_iris()
dt = pd.DataFrame(iris.data, columns=iris.feature_names)
dt.columns
X = dt[['sepal length (cm)','sepal width (cm)']].values
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
kmeans = KMeans(n_clusters=3, init='k-means++', random_state=42)
y_kmeans = kmeans.fit_predict(X_scaled)
plt.figure(figsize=(8,6))
sns.scatterplot(x=X_scaled[:, 0], y=X_scaled[:, 1], hue=y_kmeans, palette='viridis')
plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=300, c='red', marker='X', label='Centroids')
plt.xlabel('Sepal Length (Standardized)')
plt.ylabel('Sepal Width (Standardized)')
plt.title('K-Means Clustering of Iris Dataset (k=3)')
plt.legend()
plt.show()

3. Implement kmeans clustering using pyclustering.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from pyclustering.cluster.kmeans import kmeans
from pyclustering.utils import distance_metric, type_metric
from sklearn import datasets
from sklearn.preprocessing import StandardScaler
wine = datasets.load_wine()
wine_df = pd.DataFrame(wine.data, columns=wine.feature_names)
wine_df.info()
scaler = StandardScaler()
X_scaled = scaler.fit_transform(wine.data)
#convert the numpy array to list of lists
X_scaled_list = X_scaled.tolist() 
k = 3
initial_centroids = np.random.choice(len(X_scaled_list), k, replace=False)
#Covert the numpy array index values into actual data points
initial_centroids_points = [X_scaled_list[i] for i in initial_centroids]
metric = distance_metric(type_metric.EUCLIDEAN)
kmeans_instance = kmeans(X_scaled_list, initial_centroids_points)
kmeans_instance.process()
clusters = kmeans_instance.get_clusters()
centroids = kmeans_instance.get_centers()
cluster_labels = np.zeros(len(X_scaled_list))
for cluster_id, cluster in enumerate(clusters):
    for index in cluster:
        cluster_labels[index] = cluster_id
plt.figure(figsize=(8,6))
plt.scatter(X_scaled[:,0], X_scaled[:,1],c=cluster_labels,cmap='viridis',marker='o')
for centroid in centroids:
    plt.scatter(centroid[0],centroid[1],s=200,c='red',marker='X',label='Centroid')
plt.title("K-Means Clustering (Wine Dataset) with PyClustering")
plt.legend()
plt.show()

______________________________________________________________________________________

Tutorial 13 Implement K-Medoids clustering on iris dataset using pyclustering.

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from pyclustering.cluster.kmedoids import kmedoids
from pyclustering.utils import distance_metric, type_metric
from sklearn import datasets
from sklearn.preprocessing import StandardScaler
wine = datasets.load_wine()
wine_df = pd.DataFrame(wine.data, columns=wine.feature_names)
wine_df.info()
print(wine_df)
scaler = StandardScaler()
X_scaled = scaler.fit_transform(wine.data)
import random
initial_medoids = random.sample(range(len(X_scaled)),3)
metric = distance_metric(type_metric.EUCLIDEAN)
kmedoids_instance = kmedoids(X_scaled, initial_medoids, metric=metric)
kmedoids_instance.process()
clusters = kmedoids_instance.get_clusters()
medoids = kmedoids_instance.get_medoids()
plt.figure(figsize=(8,6))
for cluster in clusters:
    plt.scatter(X_scaled[cluster, 0], X_scaled[cluster, 1], label=f'Cluster {clusters.index(cluster)}')

#Mark the medoids in red
for medoid in medoids:
    plt.scatter(X_scaled[medoid, 0], X_scaled[medoid, 1],s=200,c='red',marker='X',label='Medoid')
plt.title("K-Medoids Clustering (Wine Dataset) with PyClustering")
plt.legend()
plt.show()
______________________________________________________________________________________

Tutorial 14 Implement PCA on given matrix.


Implement PCA on given matrix.
import numpy as np
#step 1: collect data

X=np.array([
    [2,3,4],
    [3,5,6],
    [5,8,9],
    [6,9,11],
    [7,10,13]
])
#step 2: compute mean value for each column
mean_X=np.mean(X,axis=0)
print(mean_X)
#step 3: center the data
X_centered=X - mean_X
print(X_centered)
#step 4: compute the covariance matrix
cov_matrix = np.cov(X_centered,rowvar=False)
print(cov_matrix)
#step 5: compute eighenvalues and eigenvector
eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)
print(eigenvalues)
print(eigenvectors)
# Step 6: Select Principal Components (Sorting by largest eigenvalues)
sorted_indices = np.argsort(eigenvalues)[::-1]
eigenvalues = eigenvalues[sorted_indices]
eigenvectors = eigenvectors[:, sorted_indices]
# Step 7: Transform the Data
# 3 to 1 (Project onto first principal component)
X_transformed_1 = X_centered @ eigenvectors[:, 0]
# 3 to 2 (Project onto first two principal components)
X_transformed_2 = X_centered @ eigenvectors[:, :2]
# Output results
print("Transformed Data (3 to 1):")
print(X_transformed_1)
print("\nTransformed Data (3 to 2):")
print(X_transformed_2)













Theory
Artificial Intelligence

 Books
– Artificial Intelligence, Third Edition, Tata-Mc Graw
Hill Author(s): E.Rich & K. Knight (text book)
– Artificial Intelligence – A New Synthesis Author(s):
Nils J. Nilsson, Tom M Mitchell by Kaufmann
Publishers.

Unit 1
Prepared by: Chhaya Patel 2

Unit 1
Introduction to Artificial
Intelligence

 Definition of an AI
– AI is subset of Data Science.
– Data science works with statistics, probability and
linear algebra.
– Artificial = man-made
– Intelligence = thinking power
– AI = a man-made thinking power

Unit 1
Prepared by: Chhaya Patel 3

Artificial Intelligence

 Definition of an AI

Unit 1
Prepared by: Chhaya Patel 4

Definition of an AI

– “AI is a branch of computer science by which we
can create intelligent machines which can behave like
a human, think like humans, and able to make
decisions.”
OR
– AI is the simulation of human intelligence processes
by machines, especially computer systems.

Unit 1
Prepared by: Chhaya Patel 5

Characteristics of AI

– AI make a machine to acquire information from
experiences.
– AI makes machine to operate humans-like-activities.
– Capable of predicting and adapting.
– Makes decisions on its own.
– AI continuous learning.

Unit 1
Prepared by: Chhaya Patel 6

Advantages of AI

– Reduction in human error
– Available 24×7
– Helps in repetitive work
– Digital assistance
– Faster decisions
– Medical applications
– Improves Security
– Efficient Communication etc..

Unit 1
Prepared by: Chhaya Patel 7

Major area of AI

Unit 1
Prepared by: Chhaya Patel 8

1. Expert Systems

– An expert system is AI software that uses knowledge
stored in a knowledge base.
– Knowledge Engineering is the term used to define the
process of building an Expert System.
– For example, the expert system provides
suggestions for spelling and errors in Google
Search Engine.

–

Unit 1
Prepared by: Chhaya Patel 9

1. Expert Systems

– It is mainly used in the medical field to operate
medical facilities and detect virus infections.
– It is also used in the banking sector for loan and
investment analysis.
– Example:
• Dendral
• Mycin
• CaDet
• DXplain

Unit 1
Prepared by: Chhaya Patel 10

2. Robotics

– Robotics is an interdisciplinary field of science and
engineering incorporated with mechanical
engineering, electrical engineering, computer
science, and many others.
– It deals with computer systems for their control,
intelligent outcomes, and information transformation.
– Robot can perform like humans, if enabled with AI.
– Example: Sophia,

Unit 1
Prepared by: Chhaya Patel 11

3. Machine Learning

– Machine learning (ML) is a type of AI that allows
software applications to become more accurate at
predicting outcomes without being explicitly
programmed to do so.

– Machine learning algorithms use historical data as input
to predict new output values.

Unit 1
Prepared by: Chhaya Patel 12

3. Machine Learning

– There are three major categories of Machine Learning:
 Supervised Learning
 Model trained with labelled dataset.
 Classification - Classification is the process of
categorizing a given set of data into classes.
 Regression - model the relationship between a
dependent (output) and independent (input)
variables with one or more independent variables.

Unit 1
Prepared by: Chhaya Patel 13

3. Machine Learning

– There are three major categories of Machine Learning:
 Un supervised Learning
 Models are trained using unlabelled dataset and are
allowed to act on that data without any supervision.
 Clustering: identify and group similar data points
in larger datasets without concern for the specific
outcome.

Unit 1
Prepared by: Chhaya Patel 14

3. Machine Learning

– There are three major categories of Machine Learning:
 Semi supervised Learning
 learn from a small amount of labeled data and train
model for large amount of unlabeled.

Unit 1
Prepared by: Chhaya Patel 15

4. Neural Network / Deep Learning

– Deep Learning is a type of machine learning based on
artificial neural networks in which multiple layers of
processing are used to extract higher level features
from data.

– This wonderful branch of AI is also responsible for
virtual assistant apps such as “Alexa and Siri”.

Unit 1
Prepared by: Chhaya Patel 16

4. Neural Network / Deep Learning

– ANN – used to solve numerical problems.
– CNN – used to solve problems with images.
– RNN – used to solve problem with time series.

Unit 1
Prepared by: Chhaya Patel 17

5. Fuzzy Logic

– Fuzzy Logic Systems (FLS) produce acceptable but
definite output in response to incomplete, ambiguous,
or inaccurate (fuzzy) input.
– Fuzzy Logic helps to offer a certain level of reasoning
flexibility when faced with uncertainties.
– Applications: Automatic Gearboxes, Four-Wheel
Steering, Vehicle environment control, Microwave
Ovens, Refrigerators, Toasters etc.

Unit 1
Prepared by: Chhaya Patel 18

6. Natural Language Processing

– NLP is a field of AI with the ability of a computer to
understand, analyze, manipulate, and potentially
generate human language.

– This is simply the process of making computer
systems to understand basic human interactions.

– Example: google assistant, Siri, Alexa, chatbots for
customer support.

Unit 1
Prepared by: Chhaya Patel 19

Application of AI

– Google’s AI-powered predictions (E.g.: Google Maps)
– Ride-sharing applications (E.g.: Uber, Lyft)
– AI Autopilot in Commercial Flights
– Spam filters on E-mails
– Plagiarism checkers and tools
– Facial Recognition
– Search recommendations
– Voice-to-text features
– Smart personal assistants (E.g.: Siri, Alexa)
– Fraud protection and prevention.

Unit 1
Prepared by: Chhaya Patel 20

Problem and Problem Solving

 To build system to solve particular problem, we need to
do 4 things:
 Define problem precisely.
 Analyze the problem.
 Represent task knowledge.
 Choose the best problem solving technique.
 Problem solving is a process of generating solutions
from the observed data.

Unit 1
Prepared by: Chhaya Patel 21

State and State Space search

 A state is a representation of problem elements at a
given moment.
 A state space is the set of all possible states reachable
from the initial state.
 A state space creates a graph in which the nodes are
states and the arcs between nodes are actions.
 In a state space, a path is a sequence of states
connected by a sequence of actions.
 The solution of a problem is a part of the graph formed
by the state space. Unit 1

Prepared by: Chhaya Patel 22

State and State Space search

Unit 1
Prepared by: Chhaya Patel 23
Chess - Initial Position Position after a legal

move

Define the Problem as State
Space Search

• A state space is formally represented as a tuple
S:⟨S, A, Action(s) ,Result(s, a) ,Cost(s, a)⟩
– S is the set of all possible states
– A is the set of possible actions
– Action(s) is possible action to perform in a certain state;
– Result(s,a) is a state reached performing action a in state s
– Cost(s,a) is the cost of performing an action a in state s.

Unit 1
Prepared by: Chhaya Patel 25

State Space Search
1 - 8 Puzzle Problem

 Problem Definition: The 8 puzzle consists of eight
numbered, movable tile set in a 3x3 frame. One cell of
the frame is always empty thus making it possible to
move an adjacent numbered tile into the empty cell.
Such a puzzle is illustrated in following diagram.

26

2 8 3

1 6 4

7 5

1 2 3

8 4

7 6 5

Initial State Goal State

State Space Search
1 - 8 Puzzle Problem

 A solution to the problem is an appropriate sequence of
moves, such as “move tiles 5 to the right, move tile 7 to
the left ,move tile 6 to the down” etc...

Unit 1
Prepared by: Chhaya Patel 27

Initial State Next State after
one legal move

2 8 3

1 6 4

7 5

2 8 3

1 4

7 6 5

28

2
8
3
1
6
4
7
5

2
8
3
1
4
7
6
5

2
3
1
8
4
7
6
5

1
2
3
8
4
7
6
5

1
2
3
8
4
7
6
5

2
8
3
1
6
4
7
5

2
8
3
1
6
4
7
5

2
8
3
1
4
7
6
5

2
8
3
1
4
7
6
5

2
8
3
7
1
4
6
5

8
3
2
1
4
7
6
5

2
3
1
8
4
7
6
5

2
3
1
8
4
7
6
5

1
2
3
7
8
4
6
5

12345

6

Unit 1
Prepared by: Chhaya Patel 29

Activity
Solve given 8 Puzzle Problem

1 2 3

8 4

7 6 5

1 2 3

6 4

8 7 5

Initial State Goal State

8-Puzzle Solver (deniz.co)

Unit 1
Prepared by: Chhaya Patel 30

Problem Definition:
– You are given two jugs, 4-ltr and 3-ltr.
– Neither jug has any measuring markings on it.
– A pump which has unlimited water which you can
use to fill the jug.
– How can you get exactly 2ltr of water in the a 4-ltr
jug?

Unit 1
Prepared by: Chhaya Patel 31

State Space Search
2 - Water Jug

– We will represent a state of the problem as a tuple (x, y)
– x = amount of water in the 4ltr jug
– y = amount of water in the 3ltr jug
1. Initial State : (0, 0) ; 0 ≤ x ≤ 4, and 0 ≤ y ≤ 3.
2. Goal state is (2, n) for any value of n.

Unit 1
Prepared by: Chhaya Patel 32

State Space Representation
2 - Water Jug

State Space Representation
2 - Water Jug

Unit 1
Prepared by: Chhaya Patel 33

Production Rules
Sr. Current state Next state Description

1 (x, y) If x<4 (4, y) fill the 4- ltr jug

2 (x, y) If y<3 (x,3) fill the 3-ltr jug

3 (x, y) If x>0 (x-d, y) pour some water out of the 4-ltr jug

4 (x, y) If y>0 (x, y-d) pour some water out of the 3-ltr jug

5 (x, y) If x>0 (0, y) empty the 4-ltr jug on the ground
6 (x, y) If y>0 (x, 0) empty the 3-ltr jug on the ground

State Space Representation
2 - Water Jug

34

Production Rules

Sr. Current state Next state Description

7 (x, y) If x + y >= 4 & y>0 (4,y-(4-x)) pour water from the 3-ltr
jug into the 4-ltr jug until
the 4-ltr jug is full

8 (x, y) If x + y >= 3 & x>0 (x-(3-y),3)) pour water from the 4-ltr
jug into the 3-ltr jug until
the 3-ltr jug is full

9 (x, y) If x + y <= 4 & y>0 (x+y,0) pour all the water from the
3-ltr jug into the 4-ltr jug

State Space Representation
2 - Water Jug

35

Production Rules

Sr. Current state Next state Description

10 (x, y) If x + y <= 3 & x>0 (0,x+y) pour all the water from the 4 -

ltr jug into the 3-ltr jug

11 (0,2) (2,0) pour the 2-ltr from the 3 – ltr jug

into the 4-ltr jug

12 (2,y) (0, y) empty the 2 ltr in the 4 ltr

on the ground

State Space Representation
2 - Water Jug

36

Productions for the water jug

4- ltr Jug 3- ltr Jug Rule Applied
0 0
0 3 2
3 0 9
3 3 2
4 2 7
0 2 5 or 12
2 0 9 or 11

State Space Representation
3 - Play Chess

 we have to specify
1. The starting position of the chess board
2. The rules that define legal moves
3. And the board position that represent a win
1. Initial Position of the chess board : The starting
position can be described by an 8 X 8 array square in
which each element square (x, y) describes the board
position of an appropriate piece in the official chess
opening position.

Unit 1
Prepared by: Chhaya Patel 37

State Space Representation
3 - Play Chess

2. The rules that define legal moves
• The legal moves provide the way of getting from initial
state of final state.
• It can be described as a set of rules consisting of two parts: A
left side that gives the current position and the right side that
describes the change to be made to the board position.

3. The board position that represents a win
• The goal is any board position in which the opponent does
not have a legal move and his or her “king” is under attack.

Unit 1
Prepared by: Chhaya Patel 38

State Space Representation
3 - Play Chess

2. legal move
Square(file e, rank 3)
is empty AND
Square(file e, rank 4)
is empty

White pawn at
Square(file e, rank 2)

Unit 1
Prepared by: Chhaya Patel 39

State Space Representation
3 - Play Chess

3. The board position that represents a win

Unit 1
Prepared by: Chhaya Patel 40

Problem Characteristics

1. Is the problem decomposable into a set of independent
smaller or easier sub-problems?
2. Can solution steps be ignored or at least undone if they
prove unwise?
3. Is the problem’s universe predictable?
4. Is a good solution to the problem obvious without
comparison to all other possible solutions?

Unit 1
Prepared by: Chhaya Patel 41

Problem Characteristics

5. Is the desired solution is a state or a path?
6. Is a large amount of knowledge required to solve the
problem or is knowledge important only to constrain
the search?
7. Can a computer that is simply given the problem return
the solution or will the solution of the problem require
interaction between the computer and a person?

Unit 1
Prepared by: Chhaya Patel 42

8 Puzzle Analysis with Respect to Seven
Problem Characteristics

Unit 1
Prepared by: Chhaya Patel 43

Problem Characteristics Satisfied Justification

Is the problem decomposable ? No Dependent moves
Can solution steps be ignored or
at least undone Yes We can undo the previous move
Is the problem’s universe
predictable Yes Problem Universe is predictable, it is a

single person game

Is a good solution absolute or
relative? Absolute Winning position need not be

compared

Is the solution a state or a path? Path Not only solution but how it is achieved

also matters

What is the role of knowledge? Domain specific knowledge is required

to constrain search

Does the task require Interaction
with a person? No In 8 puzzle additional assistance is not

required

Production System

– Production systems provide appropriate structures for
performing and describing search processes.
– A production system has four basic components:
• A set of rules each consisting of a left side that determines the
applicability of the rule and a right side that describes the
operation to be performed if the rule is applied.
• A database of current facts established during the process of
inference.

Unit 1
Prepared by: Chhaya Patel 44

Production System

• A control strategy that specifies the order in which the rules
will be compared with facts in the database and also specifies
how to resolve conflicts in selection of several rules or
selection of more facts.
• A rule applier.
– Production systems provide us with good ways of
describing the operations that can be performed in a
search for a solution to a problem.

Search techniques

Search Techniques

• Book : Artificial Intelligence, Third Edition,
Tata-Mc Graw Hill Author(s): E.Rich & K.
Knight (Chapter - 3)

• https://www.javatpoint.com/search-algorithms-
in-ai

Unit 2
Prepared by: Chhaya Patel 1

Search Techniques

• Search: Searching is a step by step procedure to
solve a search-problem in a given search space. A
search problem can have three main factors:
– Search Space: Search space represents a set of possible
solutions, which a system may have.
– Start State: It is a state from where agent begins the
search.
– Goal state: It is a function which observe the current
state and returns whether the goal state is achieved or not.

Unit 2
Prepared by: Chhaya Patel 2

Search Techniques

Unit 2
Prepared by: Chhaya Patel 3

Uninformed/Blind Search

 Uninformed search does not contain any domain
knowledge such as closeness, the location of the goal.
 It operates in a brute-force way as it only includes
information about how to traverse the tree and how to
identify leaf and goal nodes.
 Uninformed search applies a way in which search tree is
searched without any information about the search
space like initial state operators and test for the goal, so
it is also called blind search.
 It examines each node of the tree until it achieves the
goal node.

Unit 2
Prepared by: Chhaya Patel 4

Informed/Heuristic Search

 Informed Search algorithms use domain knowledge.
 In an informed search, problem information is available
which can guide the search.
 Informed search strategies can find a solution more
efficiently than an uninformed search strategy.
 Informed search is also called a Heuristic search.
 A heuristic is a way which might not always be
guaranteed for best solutions but guaranteed to find a
good solution in reasonable time.

Unit 2
Prepared by: Chhaya Patel 5

BFS - Breadth-first Search

 BFS starts searching from the root node of the tree and
expands all successor node level by level.
 BFS implemented using FIFO queue data structure.
 BFS will provide a solution if any solution exists.
 It requires lots of memory since each level of the tree
must be saved into memory to expand the next level.
 BFS needs lots of time if the solution is far away from the
root node.

Unit 2
Prepared by: Chhaya Patel 6

DFS - Depth-first Search

 DFS is a recursive algorithm for traversing a tree or graph
data structure.
 DFS starts from the root node and follows each path to its
greatest depth node before moving to the next path.
 DFS uses a stack data structure for its implementation.
 DFS requires very less memory as it only needs to store a
stack of the nodes on the path from root node to the current
node.
 There is no guarantee of finding the solution.

Unit 2
Prepared by: Chhaya Patel 7

BFS - DFS

Unit 2
Prepared by: Chhaya Patel 8

A
B C

D E F G

H
✓

✓ ✓

✓ ✓ ✓
✓

✓

A
B C

D E F G

H
✓
✓

✓

✓

✓ ✓

✓

Activity

 Reading – Writing
 Write Difference between DFS and BFS.

Unit 2
Prepared by: Chhaya Patel 9

Heuristic Search

 Every search process can be viewed as a traversal of a directed
graph.
 Nodes represent problem states and the arcs represent relationships
between states.
 Domain-specific knowledge must be added to improve search
efficiency.
 The Domain-specific knowledge about the problem includes the
nature of states, cost of transforming from one state to another, and
characteristics of the goals.
 This information can often be expressed in the form of Heuristic
Evaluation Function.

Unit 2
Prepared by: Chhaya Patel 10

Heuristic Search

 Some prominent intelligent search algorithms are stated
below:
 Generate and Test Search
 Best-first Search
 A* Search
 Constraint Search
 Means-ends analysis

Unit 2
Prepared by: Chhaya Patel 11

Generate and Test Search

 Its Heuristic search technique.
 Its DFS with Backtracking
 It is also known as British Museum algorithm.

Unit 2
Prepared by: Chhaya Patel 12

Algorithm: Generate-And-Test
1.Generate a possible solution.
2.Test to see if this is the expected
solution.
3.If the solution has been found quit else
go to step 1.

Generate and Test Search

Unit 2
Prepared by: Chhaya Patel 13

Best First Search

Unit 2
Prepared by: Chhaya Patel 14
 Best first search uses the concept of a priority queue and
heuristic search.
 It is a search algorithm that works on a specific rule.
 The aim is to reach the goal from the initial state via the shortest
path.
 Best first search combines the advantages of both DFS and BFS
into a single method.
 DFS is good because it allows a solution to be found without
expanding all competing branches. BFS is good because it does
not get trapped on dead end paths.

Best First Search

Unit 2
Prepared by: Chhaya Patel 15
 One way of combining BFS and DFS is to follow a single
path at a time, but switch paths whenever some competing
path looks more promising than the current one does.
 At each step of the Best First Search process, we select the
most promising of the nodes we have generated so far.
 We then expand the chosen node by using the rules to
generate its successors.
 If one of them is a solution, we can quit. If not, all those
new nodes are added to the set of nodes generated so far.

Best First Search - Algorithm

Unit 2
Prepared by: Chhaya Patel 16

 we assume two different list of nodes:
 OPEN list  is the list of nodes which have been
discovered but yet not expanded(whose
children/successors are yet not discovered)

 CLOSED list  nodes whose successors are also
generated.
 f(n) = h (n) where h(n) is estimated cost from current node
to the goal node

Best First Search
Algorithm

Unit 2
Prepared by: Chhaya Patel 17

Step 1: Place the starting node into the OPEN list.
Step 2: If the OPEN list is empty, Stop and return failure.
Step 3: Remove the node n, from the OPEN list which has the lowest value of h(n),
and places it in the CLOSED list.
Step 4: Expand the node n, and generate the successors of node n.
Step 5: Check each successor of node n, and find whether any node is a goal node or
not. If any successor node is goal node, then return success and terminate the search,
else proceed to Step 6.
Step 6: For each successor node, algorithm checks for evaluation function f(n), and
then check if the node has been in either OPEN or CLOSED list. If the node has not
been in both list, then add it to the OPEN list.
Step 7: Return to Step 2.

Best First Search - Example

Unit 2
Prepared by: Chhaya Patel 18

 Demonstrate Best First search for following graph.
 Path = A – B – G and Total cost = 11

Example : Find Shortest path using
best first search

Unit 2
Prepared by: Chhaya Patel 19
Path = SCDFG
Total Cost = 25

A* Best First Search

Unit 2
Prepared by: Chhaya Patel 20
 A* algorithm works based on heuristic methods and this
helps achieve optimality.
 f(n) = g(n) + h(n)
 Path = A-E-D-G

Example : Find Shortest path using
A* Best First Search

Unit 2
Prepared by: Chhaya Patel 21

Example : Find Shortest path using
A* Best First Search

Unit 2
Prepared by: Chhaya Patel 22

S

A

B

C
16 14

18

D
18

E
16

B F
31 17

Example : Find Shortest path using
A* Best First Search

Unit 2
Prepared by: Chhaya Patel 23

S

A

B

C
16 14

18

D
18

E

F

24

G
19

16

17

D

Example : Find Shortest path using
A* Best First Search

Unit 2
Prepared by: Chhaya Patel 24

S

A

B

C
16 14

18

D
18

E

F

G
19

B F
36 23

16

17

Example : Find Shortest path using
A* Best First Search

Unit 2
Prepared by: Chhaya Patel 25

S

A

B

C
16 14

18

E

F

19

D E
14 15

C F
22 19

G

D

16

17

Example : Find Shortest path using
A* Best First Search

Unit 2
Prepared by: Chhaya Patel 26

S

A

B

C
16 14

18

E

F

19

D E
14 15

A F
27

16

D G
23 18

G
16

17

A* Algorithm

Unit 2
Prepared by: Chhaya Patel 27

 Step1: Place the starting node in the OPEN list.
 Step 2: Check if the OPEN list is empty or not, if the list is empty then return
failure and stops.
 Step 3: Select the node from the OPEN list which has the smallest value of
evaluation function (g+h), if node n is goal node then return success and stop,
otherwise
 Step 4: Expand node n and generate all of its successors, and put n into the closed
list. For each successor n', check whether n' is already in the OPEN or CLOSED
list, if not then compute evaluation function for n' and place into Open list.
 Step 5: Else if node n' is already in OPEN and CLOSED, then it should be
attached to the back pointer which reflects the lowest g(n') value.
 Step 6: Return to Step 2.

Problem Decomposition

Unit 2
Prepared by: Chhaya Patel 28
 When a problem can be divided into a set of sub
problems, where each sub problem can be solved
separately and a combination of these will be a solution.
 It use AND-OR graph to find more than one solution.


AO* Algorithm

Unit 2
Prepared by: Chhaya Patel 29

 AO* is informed search based on BFS.
 It is based on problem decomposition.
 It use AND-OR graph to find more than one solutions.
 AND-OR graph is useful for representing the solution of
problems that can be solved by decomposing them into
set of smaller problems.
 All of which must be solved.

AO* Algorithm

Unit 2
Prepared by: Chhaya Patel 30
 Node in AO* algorithm will point both down to its
successors and up to its parent nodes. (backtracking).
 Each node in graph will have heuristics value associated
with it.
 Cost function f(n) = g(n) + h(n)
 g(n) = actual cost / edge cost
 h(n) = heuristics cost
 Here assume g(n) = 1

AO* Algorithm – Example-1

Unit 2
Prepared by: Chhaya Patel 31

Cost = Min (18,15)

= 15

AO* Algorithm – Example-2

Unit 2
Prepared by: Chhaya Patel 32

Cost = Min (12,16)

= 12

AO* Algorithm

Unit 2
Prepared by: Chhaya Patel 33

Constraint Satisfaction Problem

Unit 2
Prepared by: Chhaya Patel 34
CSP - is a problem that requires its solution within some
limitations or conditions also known as constraints.
It consists of the following:
1. Variables: which stores the solution (finite set)

(V = {V1, V2, V3,....., Vn})

2. Domain: set of discrete values known as domain from
which the solution is picked (D = {D1, D2, D3,.....,Dn})
3. Constraints: (C = {C1, C2, C3,......, Cn})

Constraint Satisfaction Problem

Unit 2
Prepared by: Chhaya Patel 35

Example:
– Cryptarithmetic problem
– Crossword puzzle
– Sudoku game
• fill the empty squares with numbers ranging from 1 to 9 in such a
way that no row, column or a block has a number repeating
itself.
• consider the Sudoku problem again. Suppose that a row, column
and block already have 3, 5 and 7 filled in. Then the domain for
all the variables in that row, column and block will be {1, 2, 4, 6,
8, 9}.

Unit 2
Prepared by: Chhaya Patel 36

Crypt-arithmetic Puzzle
Initial state:
• Assign values between 0 to 9.
• No two letters have the same value.
• The sum of the digits must be as shown.

1

1
9

0

S
E
N
D
M
O
R
Y

1

1

9

0

0

5

6

5
6

5

5

6

8

7

2

1

7

8
2

Unit 2
Prepared by: Chhaya Patel 37

More Examples of
Crypt-arithmetic Puzzle

Unit 2
Prepared by: Chhaya Patel 38

More Examples of
Crypt-arithmetic Puzzle

Unit 2
Prepared by: Chhaya Patel 39

Means-Ends Analysis

 MEA is mixture of the two directions forward /
Backward is appropriate for solving a complex and
large problem.
 It first solve the major part of a problem and then go
back and solve the small problems arise during
combining the big parts of the problem. Such a
technique is called Means-Ends Analysis.
 The MEA analysis process centered on the evaluation
of the difference between the current state and goal
state.

Unit 2
Prepared by: Chhaya Patel 40

Means-Ends Analysis

 Steps of MEA
1. First, evaluate the difference between Initial State and final
State.
2. Select the various operators which can be applied for each
difference.
3. Apply the operator at each difference, which reduces the
difference between the current state and goal state.

Unit 2
Prepared by: Chhaya Patel 41

Means-Ends Analysis

 Example

Unit 2
Prepared by: Chhaya Patel 42

Means-Ends Analysis

 Solution
 we will first find the differences between initial states
and goal states, and for each difference, we will
generate a new state and will apply the operators. The
operators we have for this problem are:
 Move
 Delete
 Expand

Unit 2
Prepared by: Chhaya Patel 43

Means-Ends Analysis

 Solution

Unit 2
Prepared by: Chhaya Patel 44

Means-Ends Analysis

 But sometimes it is possible that an operator cannot be
applied to the current state.
 So, we create the sub-problem of the current state, in
which operator can be applied, such type of backward
chaining in which operators are selected, and then sub
goals are set up to establish the preconditions of the
operator is called Operator Subgoaling.

Unit 2
Prepared by: Chhaya Patel 45

Means-Ends Analysis

Hill Climbing Algorithm

Unit 2
Prepared by: Chhaya Patel 47
 Hill climbing algorithm is a local search algorithm which
continuously moves in the direction of increasing value to
find the best solution to the problem.

 It is also called greedy local search as it only looks to its
good immediate neighbor state and not beyond that.

 A node of hill climbing algorithm has two components
which are state and value.

Features of Hill Climbing Algorithm

Unit 2
Prepared by: Chhaya Patel 48
 Generate and Test variant: It takes the feedback from
the test procedure. Then this feedback is utilized by the
generator in deciding the next move in search space.

 Greedy approach: Hill-climbing algorithm search moves
in the direction which optimizes the cost.

 No backtracking: It does not backtrack the search space,
as it does not remember the previous states.

Types of Hill Climbing

Unit 2
Prepared by: Chhaya Patel 49
1. Simple Hill climbing: It examines the neighboring nodes
one by one and selects the first neighboring node which
optimizes the current cost as the next node.
2. Steepest-Ascent Hill climbing: It first examines all the
neighboring nodes and then selects the node closest to the
solution state as of the next node.

In simple hill climbing, the first closer node is chosen, whereas in
steepest ascent hill climbing all successors are compared and the

closest to the solution is chosen.

Block World problem using
Simple Hill Climbing

Unit 2
Prepared by: Chhaya Patel 50

 World consists of
 A flat surface such as a table.
 A set of identical blocks which are identified by letters.
 The blocks can be stacked one on one to form towers.
 Stacking is achieved using a robot arm which has
fundamental operations and states.
 The robot can hold one block at a time and only one block
can be moved at a time.

Block World problem using
Simple Hill Climbing

Unit 2
Prepared by: Chhaya Patel 51
B
C
D

A
B
C
A D

Local heuristic:
+1 for each block that is resting on the thing it is supposed to be resting on.
-1 for each block that is resting on a wrong thing.

initial state h=0 goal state h=4
+1
-1
+1
-1 +1
+1
+1

+1

Unit 2
Prepared by: Chhaya Patel 52

B
C
D A
B
C

Start

Goal

A

D

h=0

h=4

B
C
D
A

B
C D

A B
C

A D

h=0

h=0

h=0

B
C
D

A

h=2

+1

+1
+1

-1

Block World problem using
Simple Hill Climbing

Unit 2
Prepared by: Chhaya Patel 53

Block World problem using
Hill Climbing – Local Maxima

Drawbacks of Hill Climbing

Unit 2
Prepared by: Chhaya Patel 54

Drawbacks of Hill Climbing

Unit 2
Prepared by: Chhaya Patel 55
 Local Maxima: a local maximum is a state that is better than all its
neighbors but is not better than some other states further away.
 To overcome local maximum problem: Utilize backtracking
technique. Maintain a list of visited states and explore a new path.

 Plateau (Flat): a plateau is a flat area of the search space in which, a
whole set of neighboring states have the same values.
 To overcome plateaus: Make a big jump. Randomly select a state far
away from current state.

 Ridge: is a special kind of local maximum. It is an area of the search
space that is higher than surrounding areas and that itself has slop.
 To overcome Ridge: In this kind of obstacle, use two or more rules
before testing. It implies moving in several directions at once.

Block World problem using
Steepest-Ascent Hill climbing

Unit 2
Prepared by: Chhaya Patel 56
B
C
D

A
B
C
Start Goal A D

Global heuristic:
For each block that has the correct support structure:

+1 to every block in the support structure.

For each block that has a wrong support structure:

-1 to every block in the support structure.
h=-6 h=6

+1
+2
+3

0

-1
-2
-3

0

Unit 2
Prepared by: Chhaya Patel 57

B
C
D

A
B
C

A

D

h=-6

h=6

B
C
D
A

B
C D

A B
C

A D

h=-6

h=-2

h=-1

B
C
D

A

h=-3

Block World problem using
Steepest-Ascent Hill climbing

Simple Hill Climbing - Algorithm

Unit 2
Prepared by: Chhaya Patel 58
1. Evaluate the initial state. If it is also goal state, then return it and
quit. Otherwise continue with the initial state as the current state.
2. Loop until a solution is found or until there are no new operators
left to be applied in the current state:
a. Select an operator that has not yet been applied to the current
state and apply it to produce a new state.
b. Evaluate the new state

i. If it is the goal state, then return it and quit.
ii. If it is not a goal state but it is better than the current state,
then make it the current state.
iii. If it is not better than the current state, then continue in the
loop.

Steepest-Ascent Hill Climbing
Algorithm

Unit 2
Prepared by: Chhaya Patel 59
1. Evaluate the initial state. If it is also a goal state, then return it and
quit. Otherwise, continue with the initial state as the current state.
2. Loop until a solution is found or until a complete iteration produces
no change to current state:
a. Let S be a state such that any possible successor of the current state
will be better than S.
b. For each operator that applies to the current state do:

Apply the operator and generate a new state
Evaluate the new state. If it is a goal state, then return it and quit.
If not, compare it to S. If it is better, then set S to this state. If it
is not better, leave S alone.

c. If the S is better than the current state, then set current state to S.


Introduction to Machine Learning

Prepared by: Chhaya Patel 1

Types of Data

 Structured data: is data with a high degree of organization,
usually stored in some sort of spreadsheet or table. Example: Excel
sheet, SQL database in a table that consists of columns and rows.
 Semi-structured: is data which has some degree of organization
in it. It is not as rigorously structured as structured data, but also
not as messy as unstructured data. Example: HTML file, JSON
file, XML file.
 Unstructured data: with no predefined organizational form and
no specific format. Example: image file, logo, video file, PDF file.

Prepared by: Chhaya Patel 2

Types of Data in Statistics

Prepared by: Chhaya Patel 3

Types of Data in Statistics

 Qualitative Data (Categorical data)
 Categorical data is a collection of information that is divided into groups.
 Bio data of employee, student class division etc.
 There are two types of categorical data
 Nominal data
 This is a type of data used to name variables without providing any
numerical value. Example: Gender, hair color etc.
 Ordinal data.
 This is a data type with a set order or scale to it. Example: grade of marks,
economicalstatus.

Prepared by: Chhaya Patel 4

Types of Data in Statistics

 Quantitative data (Numerical data)
 Quantitative data can be expressed as a number or can be quantified.
 it can be measured by numerical variables.
 There are two types of Numerical data
 Discrete data (Countable)
 Discrete data is a count that involves only integers. The discrete values
cannot be subdivided into parts.
 Example: no. ofstudentsin class, no. of workers in company

Prepared by: Chhaya Patel 5

Types of Data in Statistics

 Quantitative data (Numerical data)
 Continuous data (Measurable)
 Continuous data is information that could be meaningfully divided into finer
levels.
 It can be measured on a scale.
 The continuous variables can take any value between two numbers.
 you can measure your height at very precise scales.
 Example: width, temperature,time

Prepared by: Chhaya Patel 6

Types of Data in Statistics

 Find type of data
• What is your household income?
– Below 5,00,000 or 5,00,001 – 10,00,000 or above
– Answer: Nominal data
• What is your hair color?
– nominal data.
• What is your gender?
– Answer: Nominal data

Prepared by: Chhaya Patel 7

Types of Data in Statistics

 Find type of data
• Customer satisfaction data
– Answer: ordinal data
• What is your highest level of education?
– School or High School or B.Tech, M.Tech, Ph.D
– Answer: Nominal data
• Happiness level: 1-5
– Answer: ordinal data

Prepared by: Chhaya Patel 8

Types of Data in Statistics

 Find type of data
– Marital status
• Nominal Data
– The height of children
• continuous data
– The square footage of a two-bedroom house
• continuous data
– The number of workers in a company
• discrete data

Prepared by: Chhaya Patel 9

What is Machine Learning?

 Machine Learning
 ML is a type of AI that allows software applications to
become more accurate at predicting outcomes without
being explicitly programmed to do so.
 OR
 Define machine learning as a set of methods that can
automatically detect patterns in data, and then use the
uncovered patterns to predict future data.

Prepared by: Chhaya Patel 10

Types of machine learning

Prepared by: Chhaya Patel 11

Supervised Learning

 Supervised Learning
 Supervised machine learning is defined by its use of labelled datasets to train
algorithms that is used to classify data or predict outcomes accurately.
 Supervised learning uses a training data set to teach models and get desired
output.
 Training dataset includes inputs and correct outputs, which allow the model
to learn over time.
 Input data is fed into the model, it adjusts its weights until the model has
been fitted appropriately, which occurs as part of the cross validation
process.
 The algorithm measures its accuracy through the loss function, adjusting
until the error has been sufficiently minimized.

Prepared by: Chhaya Patel 12

Classification

 Classification:
 Classification is the process of categorizing a given set of data into classes.
 It takes labelled input data, which means it contains input with the
corresponding output.
 In Classification, a program learns from the given dataset or observations
and then classifies new observation into a number of classes or groups. Such
as, Yes or No, 0 or 1, Spam or Not Spam, cat or dog, etc.
 Classes can be called as targets/labels or categories.
 It performs on both structured data and unstructured data.

Prepared by: Chhaya Patel 13

Classification

 Classification:
 The algorithm which implements the classification on a dataset is known as
a classifier.
 y=f(x), where y = categorical output and x is input.
 There are two types of Classifications:
 Binary Classifier:
 If the classification problem has only two possible outcomes, then it is called
as Binary Classifier.
 Examples: YES or NO, MALE or FEMALE, SPAM or NOT SPAM, CAT
or DOG, etc.

Prepared by: Chhaya Patel 14

Classification

 Classification:
 Multi-classClassifier:
 If a classification problem has more than two outcomes, then it is called as
Multi-classClassifier.
 Example: Classifications of types of crops, Classification of types of music.

Prepared by: Chhaya Patel 15

Classification

 Use cases of Classification Algorithms
 Email Spam Detection
 Speech Recognition
 Identifications of Cancer tumor cells.
 Drugs Classification
 Biometric Identification, etc.

Prepared by: Chhaya Patel 16

Regression

 Regression
 Regression is used to understand the relationship between dependent and
independent variables.
 Regression is usually defined as determining relationships between two or
more variables.
 It is a continuous prediction problem. (Stocks increase or decrease in Stock
Exchange, Climate changes after 3 years).
 Linear Regression is used for solving Regression problems.

Prepared by: Chhaya Patel 17

Regression

 Linear Regression
 The Linear Regression Algorithm provides the relation between an
independent(input) and a dependent(output) variable.
 It demonstrates the impact on the dependent variable when the independent
variable is changed in any way.
 An example of the Linear Regression Algorithm usage is to analyze the
property prices in the area according to the size of the property, number of
rooms, etc.
 Simple linear regression - Single input variable (x)
 Multiple linear regression - Multiple input variable (x1,x2,x3.....)
